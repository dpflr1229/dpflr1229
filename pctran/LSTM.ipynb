{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fc909df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫 번째 GPU를 사용하도록 설정\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(physical_devices[0], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7e0eccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_columns = ['DNBR', 'DTHY', 'DWB', 'HLW', 'HTR', 'HUP', 'LSGA', 'LSGB', 'LVCR',\n",
    "'LVPZ', 'LWRB', 'NSGA', 'NSGB', 'P', 'PPM', 'PRB', 'PRBA', 'PSGA',\n",
    "       'PSGB', 'PWNT', 'PWR', 'QMGA', 'QMGB', 'QMWT', 'RBLK', 'RC131', 'RC87',\n",
    "       'RH', 'RHBR', 'RHFL', 'RHMT', 'RHRD', 'RM1', 'RM2', 'RM3', 'RM4',\n",
    "       'SCMA', 'SCMB', 'SGLK', 'STRB', 'STSG', 'STTB', 'TAVG', 'TBLD', 'TCA',\n",
    "       'TCB', 'TF', 'TFPK', 'TFSB', 'THA', 'THB', 'TIME', 'TPCT', 'TRB',\n",
    "       'TSAT', 'VOID', 'VOL', 'WBK', 'WCFT', 'WCHG', 'WCSP', 'WECS', 'WFWA',\n",
    "       'WFWB', 'WHPI', 'WLR', 'WRCA', 'WRCB', 'WRLA', 'WRLB', 'WSPY', 'WSTA',\n",
    "       'WSTB', 'WTRA', 'WTRB', 'WUP', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b82f11dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_columns2 = ['DNBR', 'DTHY', 'DWB', 'HLW', 'HTR', 'HUP', 'LSGA', 'LSGB', 'LVCR',\n",
    "'LVPZ', 'LWRB', 'NSGA', 'NSGB', 'P', 'PPM', 'PRB', 'PRBA', 'PSGA',\n",
    "       'PSGB', 'PWNT', 'PWR', 'QMGA', 'QMGB', 'QMWT', 'RBLK', 'RC131', 'RC87',\n",
    "       'RH', 'RHBR', 'RHFL', 'RHMT', 'RHRD', 'RM1', 'RM2', 'RM3', 'RM4',\n",
    "       'SCMA', 'SCMB', 'SGLK', 'STRB', 'STSG', 'STTB', 'TAVG', 'TBLD', 'TCA',\n",
    "       'TCB', 'TF', 'TFPK', 'TFSB', 'THA', 'THB', 'TIME', 'TPCT', 'TRB',\n",
    "       'TSAT', 'VOID', 'VOL', 'WBK', 'WCFT', 'WCHG', 'WCSP', 'WECS', 'WFWA',\n",
    "       'WFWB', 'WHPI', 'WLR', 'WRCA', 'WRCB', 'WRLA', 'WRLB', 'WSPY', 'WSTA',\n",
    "       'WSTB', 'WTRA', 'WTRB', 'WUP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff186347",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "806it [00:12, 64.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# 모듈 불러오기\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "pathList = []\n",
    "##1. 패스 생성\n",
    "homePath = 'train/' #들여다볼 폴더들이 있는 상위 폴더\n",
    "folders = os.listdir(homePath) #불러올 각 폴더들을 리스트에 저장합니다.\n",
    "\n",
    "for folder in (folders): #불러올 폴더가 여러개이니 반복문을 이용합니다.\n",
    "    folderName = folder #folderName에 각 폴더명을 저장합니다.\n",
    "\n",
    "    files = os.listdir(homePath + folder) #불러올 각 파일들을 리스트에 저장합니다.\n",
    "\n",
    "    for file in files: #불러올 파일 또한 여러개이니 반복문을 이용합니다.\n",
    "        fileName = file #fileName에 각 파일명을 저장합니다.\n",
    "\n",
    "        path = homePath + folderName + '/' + fileName #알아낸 폴더와 파일을 이름을 조합합니다.\n",
    "\n",
    "        pathList.append(path) #path들을 한 리스트에 저장\n",
    "        \n",
    "# csv 불러오기\n",
    "dfs=[]\n",
    "def load_csv(pathList):\n",
    "    for a,file in tqdm(enumerate(pathList)):\n",
    "        df = pd.read_csv(file)\n",
    "        df['label'] = pathList[a].split('/')[1]\n",
    "        dfs.append(df)\n",
    "    return dfs\n",
    "load_csv(pathList)\n",
    "\n",
    "\n",
    "train = pd.concat([df[common_columns]for df in dfs], axis=0, ignore_index=True)\n",
    "# 변수 WPCS, WPMU, WPFW 칼럼 제거\n",
    "train = train.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64953b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "206it [00:03, 64.64it/s]\n"
     ]
    }
   ],
   "source": [
    "pathList = []\n",
    "##1. 패스 생성\n",
    "homePath = 'test/' #들여다볼 폴더들이 있는 상위 폴더\n",
    "folders = os.listdir(homePath) #불러올 각 폴더들을 리스트에 저장합니다.\n",
    "\n",
    "for folder in (folders): #불러올 폴더가 여러개이니 반복문을 이용합니다.\n",
    "    folderName = folder #folderName에 각 폴더명을 저장합니다.\n",
    "\n",
    "    files = os.listdir(homePath + folder) #불러올 각 파일들을 리스트에 저장합니다.\n",
    "\n",
    "    for file in files: #불러올 파일 또한 여러개이니 반복문을 이용합니다.\n",
    "        fileName = file #fileName에 각 파일명을 저장합니다.\n",
    "\n",
    "        path = homePath + folderName + '/' + fileName #알아낸 폴더와 파일을 이름을 조합합니다.\n",
    "\n",
    "        pathList.append(path) #path들을 한 리스트에 저장\n",
    "        \n",
    "# csv 불러오기\n",
    "dfs=[]\n",
    "load_csv(pathList)\n",
    "\n",
    "test = pd.concat([df[common_columns]for df in dfs], axis=0, ignore_index=True)\n",
    "# 변수 WPCS, WPMU, WPFW 칼럼 제거\n",
    "test = test.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d15df0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단일 : Normal, SP, LACP, LOF, ATWS, TT  \n",
    "idx = train[(train['label']  == 'LOF') | (train['label']  == 'NORM') | (train['label'] == 'SP') | (train['label'] == 'LACP') | (train['label'] == 'ATWS') | (train['label'] == 'TT')].index\n",
    "train.drop(idx, axis = 0, inplace=True)\n",
    "idx = test[(test['label']  == 'LOF') | (test['label']  == 'NORM') | (test['label'] == 'SP') | (test['label'] == 'LACP') | (test['label'] == 'ATWS') | (test['label'] == 'TT')].index\n",
    "test.drop(idx, axis = 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3903d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 축소\n",
    "train = train[train['TIME'] <= 3500]\n",
    "test = test[test['TIME'] <= 3500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c6acc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30초 간격으로 데이터 축소해보기\n",
    "#trian = train[train['TIME'] % 30 == 0]\n",
    "train.reset_index(drop=True, inplace = True)\n",
    "#test = test[test['TIME'] % 30 == 0]\n",
    "test.reset_index(drop=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "296db74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코딩\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "train['label'] = le.fit_transform(train['label'])\n",
    "test['label'] = le.fit_transform(test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ee574bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['FLB', 'LLB', 'LOCA', 'LOCAC', 'LR', 'MD', 'SGATR', 'SGBTR',\n",
       "       'SLBIC', 'SLBOC'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bdc7e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DNBR</th>\n",
       "      <th>DTHY</th>\n",
       "      <th>DWB</th>\n",
       "      <th>HLW</th>\n",
       "      <th>HTR</th>\n",
       "      <th>HUP</th>\n",
       "      <th>LSGA</th>\n",
       "      <th>LSGB</th>\n",
       "      <th>LVCR</th>\n",
       "      <th>LVPZ</th>\n",
       "      <th>...</th>\n",
       "      <th>WRCB</th>\n",
       "      <th>WRLA</th>\n",
       "      <th>WRLB</th>\n",
       "      <th>WSPY</th>\n",
       "      <th>WSTA</th>\n",
       "      <th>WSTB</th>\n",
       "      <th>WTRA</th>\n",
       "      <th>WTRB</th>\n",
       "      <th>WUP</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.630000</td>\n",
       "      <td>0.009256</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>1310.893433</td>\n",
       "      <td>200.000006</td>\n",
       "      <td>2583.701416</td>\n",
       "      <td>10.957693</td>\n",
       "      <td>10.957693</td>\n",
       "      <td>7.924896</td>\n",
       "      <td>52.600002</td>\n",
       "      <td>...</td>\n",
       "      <td>27556.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2873.244873</td>\n",
       "      <td>2873.244873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.629482</td>\n",
       "      <td>0.009256</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>1310.863403</td>\n",
       "      <td>197.180080</td>\n",
       "      <td>2583.630371</td>\n",
       "      <td>10.957623</td>\n",
       "      <td>10.957623</td>\n",
       "      <td>7.924896</td>\n",
       "      <td>52.618706</td>\n",
       "      <td>...</td>\n",
       "      <td>27556.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2873.244873</td>\n",
       "      <td>2873.244873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.431455</td>\n",
       "      <td>0.009256</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>1339.402344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2584.622559</td>\n",
       "      <td>11.082749</td>\n",
       "      <td>11.080099</td>\n",
       "      <td>7.924896</td>\n",
       "      <td>52.107727</td>\n",
       "      <td>...</td>\n",
       "      <td>22063.697266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.246471</td>\n",
       "      <td>-40.246471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.102180</td>\n",
       "      <td>0.223086</td>\n",
       "      <td>0.004422</td>\n",
       "      <td>1344.995361</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2586.268311</td>\n",
       "      <td>10.990372</td>\n",
       "      <td>10.986485</td>\n",
       "      <td>7.924896</td>\n",
       "      <td>51.269459</td>\n",
       "      <td>...</td>\n",
       "      <td>16787.556641</td>\n",
       "      <td>1368.889648</td>\n",
       "      <td>1368.324463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1423.972412</td>\n",
       "      <td>1313.241821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.274228</td>\n",
       "      <td>0.214693</td>\n",
       "      <td>0.004461</td>\n",
       "      <td>1330.819946</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2593.570068</td>\n",
       "      <td>10.723519</td>\n",
       "      <td>10.719125</td>\n",
       "      <td>7.924896</td>\n",
       "      <td>47.594593</td>\n",
       "      <td>...</td>\n",
       "      <td>13054.722656</td>\n",
       "      <td>1352.285767</td>\n",
       "      <td>1351.897095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1396.430786</td>\n",
       "      <td>1307.752197</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560795</th>\n",
       "      <td>1.530014</td>\n",
       "      <td>0.009256</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>1295.781860</td>\n",
       "      <td>2.111266</td>\n",
       "      <td>2578.666504</td>\n",
       "      <td>10.965378</td>\n",
       "      <td>10.966446</td>\n",
       "      <td>7.924896</td>\n",
       "      <td>53.888554</td>\n",
       "      <td>...</td>\n",
       "      <td>27710.248047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3042.019287</td>\n",
       "      <td>3096.377441</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560796</th>\n",
       "      <td>1.529903</td>\n",
       "      <td>0.009256</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>1295.825195</td>\n",
       "      <td>2.179616</td>\n",
       "      <td>2578.668213</td>\n",
       "      <td>10.965837</td>\n",
       "      <td>10.966155</td>\n",
       "      <td>7.924896</td>\n",
       "      <td>53.887630</td>\n",
       "      <td>...</td>\n",
       "      <td>27710.248047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3057.864014</td>\n",
       "      <td>3080.520020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560797</th>\n",
       "      <td>1.529973</td>\n",
       "      <td>0.009256</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>1295.806519</td>\n",
       "      <td>1.986283</td>\n",
       "      <td>2578.663330</td>\n",
       "      <td>10.965630</td>\n",
       "      <td>10.966139</td>\n",
       "      <td>7.924896</td>\n",
       "      <td>53.890182</td>\n",
       "      <td>...</td>\n",
       "      <td>27710.248047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3067.846191</td>\n",
       "      <td>3070.492676</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560798</th>\n",
       "      <td>1.529964</td>\n",
       "      <td>0.009256</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>1295.840210</td>\n",
       "      <td>2.249919</td>\n",
       "      <td>2578.669922</td>\n",
       "      <td>10.965598</td>\n",
       "      <td>10.966095</td>\n",
       "      <td>7.924896</td>\n",
       "      <td>53.886688</td>\n",
       "      <td>...</td>\n",
       "      <td>27710.248047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3070.744141</td>\n",
       "      <td>3067.656738</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560799</th>\n",
       "      <td>1.529913</td>\n",
       "      <td>0.009256</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>1295.782471</td>\n",
       "      <td>1.994095</td>\n",
       "      <td>2578.663818</td>\n",
       "      <td>10.965000</td>\n",
       "      <td>10.966609</td>\n",
       "      <td>7.924896</td>\n",
       "      <td>53.890083</td>\n",
       "      <td>...</td>\n",
       "      <td>27710.248047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3032.624023</td>\n",
       "      <td>3105.847412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>560800 rows × 77 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            DNBR      DTHY       DWB          HLW         HTR          HUP  \\\n",
       "0       1.630000  0.009256  0.000692  1310.893433  200.000006  2583.701416   \n",
       "1       1.629482  0.009256  0.000692  1310.863403  197.180080  2583.630371   \n",
       "2       2.431455  0.009256  0.000692  1339.402344    0.000000  2584.622559   \n",
       "3       4.102180  0.223086  0.004422  1344.995361    0.000000  2586.268311   \n",
       "4       6.274228  0.214693  0.004461  1330.819946    0.000000  2593.570068   \n",
       "...          ...       ...       ...          ...         ...          ...   \n",
       "560795  1.530014  0.009256  0.000692  1295.781860    2.111266  2578.666504   \n",
       "560796  1.529903  0.009256  0.000692  1295.825195    2.179616  2578.668213   \n",
       "560797  1.529973  0.009256  0.000692  1295.806519    1.986283  2578.663330   \n",
       "560798  1.529964  0.009256  0.000692  1295.840210    2.249919  2578.669922   \n",
       "560799  1.529913  0.009256  0.000692  1295.782471    1.994095  2578.663818   \n",
       "\n",
       "             LSGA       LSGB      LVCR       LVPZ  ...          WRCB  \\\n",
       "0       10.957693  10.957693  7.924896  52.600002  ...  27556.000000   \n",
       "1       10.957623  10.957623  7.924896  52.618706  ...  27556.000000   \n",
       "2       11.082749  11.080099  7.924896  52.107727  ...  22063.697266   \n",
       "3       10.990372  10.986485  7.924896  51.269459  ...  16787.556641   \n",
       "4       10.723519  10.719125  7.924896  47.594593  ...  13054.722656   \n",
       "...           ...        ...       ...        ...  ...           ...   \n",
       "560795  10.965378  10.966446  7.924896  53.888554  ...  27710.248047   \n",
       "560796  10.965837  10.966155  7.924896  53.887630  ...  27710.248047   \n",
       "560797  10.965630  10.966139  7.924896  53.890182  ...  27710.248047   \n",
       "560798  10.965598  10.966095  7.924896  53.886688  ...  27710.248047   \n",
       "560799  10.965000  10.966609  7.924896  53.890083  ...  27710.248047   \n",
       "\n",
       "               WRLA         WRLB  WSPY         WSTA         WSTB  WTRA  WTRB  \\\n",
       "0          0.000000     0.000000   0.0  2873.244873  2873.244873   0.0   0.0   \n",
       "1          0.000000     0.000000   0.0  2873.244873  2873.244873   0.0   0.0   \n",
       "2          0.000000     0.000000   0.0    40.246471   -40.246471   0.0   0.0   \n",
       "3       1368.889648  1368.324463   0.0  1423.972412  1313.241821   0.0   0.0   \n",
       "4       1352.285767  1351.897095   0.0  1396.430786  1307.752197   0.0   0.0   \n",
       "...             ...          ...   ...          ...          ...   ...   ...   \n",
       "560795     0.000000     0.000000   0.0  3042.019287  3096.377441   0.0   0.0   \n",
       "560796     0.000000     0.000000   0.0  3057.864014  3080.520020   0.0   0.0   \n",
       "560797     0.000000     0.000000   0.0  3067.846191  3070.492676   0.0   0.0   \n",
       "560798     0.000000     0.000000   0.0  3070.744141  3067.656738   0.0   0.0   \n",
       "560799     0.000000     0.000000   0.0  3032.624023  3105.847412   0.0   0.0   \n",
       "\n",
       "        WUP  label  \n",
       "0       0.0      0  \n",
       "1       0.0      0  \n",
       "2       0.0      0  \n",
       "3       0.0      0  \n",
       "4       0.0      0  \n",
       "...     ...    ...  \n",
       "560795  0.0      9  \n",
       "560796  0.0      9  \n",
       "560797  0.0      9  \n",
       "560798  0.0      9  \n",
       "560799  0.0      9  \n",
       "\n",
       "[560800 rows x 77 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e46504d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe1c0c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e3967d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.iloc[:, :-1].values  # 마지막 열을 제외한 모든 열 (특징)\n",
    "y = train.iloc[:, -1].values  # 마지막 열 (타겟)\n",
    "X_test = test.iloc[:, :-1].values  # 마지막 열을 제외한 모든 열 (특징)\n",
    "y_test = test.iloc[:, -1].values  # 마지막 열 (타겟)\n",
    "\n",
    "#데이터 정규화\n",
    "scaler = StandardScaler()\n",
    "#scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "# 데이터를 PyTorch 텐서로 변환\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_train = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y, dtype=torch.long).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a99a122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 데이터로더 준비\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a5b58cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_lstm_layers=2, dropout_prob=0.5):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_lstm_layers, batch_first=True, dropout=dropout_prob, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_size*2, 64)  # 첫 번째 FC 레이어\n",
    "        self.batch_norm = nn.BatchNorm1d(64)  # 배치 정규화\n",
    "        self.fc2 = nn.Linear(64, num_classes)  # 출력 레이어 (클래스 수)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        \n",
    "        # LSTM 층으로 데이터 통과\n",
    "        x = x.unsqueeze(1)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        \n",
    "        # 마지막 시점의 LSTM 출력을 사용\n",
    "        x = lstm_out[:, -1, :]\n",
    "        \n",
    "        # 드롭아웃 적용\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 첫 번째 FC 레이어\n",
    "        x = self.fc1(x)\n",
    "        x = self.batch_norm(x)  # 배치 정규화\n",
    "        x = torch.relu(x)  # ReLU 활성화 함수\n",
    "        \n",
    "        # 두 번째 FC 레이어\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 128\n",
    "num_classes = len(np.unique(y))  # 클래스 수\n",
    "\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57210080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 2.4674, Accuracy: 0.0719\n",
      "Epoch [2/100], Loss: 2.3958, Accuracy: 0.0878\n",
      "Epoch [3/100], Loss: 2.3807, Accuracy: 0.0884\n",
      "Epoch [4/100], Loss: 2.3696, Accuracy: 0.0898\n",
      "Epoch [5/100], Loss: 2.3484, Accuracy: 0.1078\n",
      "Epoch [6/100], Loss: 2.2968, Accuracy: 0.1830\n",
      "Epoch [7/100], Loss: 2.2239, Accuracy: 0.2540\n",
      "Epoch [8/100], Loss: 2.1372, Accuracy: 0.3127\n",
      "Epoch [9/100], Loss: 2.0370, Accuracy: 0.3987\n",
      "Epoch [10/100], Loss: 1.9526, Accuracy: 0.4521\n",
      "Epoch [11/100], Loss: 1.8290, Accuracy: 0.5460\n",
      "Epoch [12/100], Loss: 1.6994, Accuracy: 0.6216\n",
      "Epoch [13/100], Loss: 1.5708, Accuracy: 0.6699\n",
      "Epoch [14/100], Loss: 1.4355, Accuracy: 0.7062\n",
      "Epoch [15/100], Loss: 1.2720, Accuracy: 0.7468\n",
      "Epoch [16/100], Loss: 1.1058, Accuracy: 0.7808\n",
      "Epoch [17/100], Loss: 0.9409, Accuracy: 0.8063\n",
      "Epoch [18/100], Loss: 0.8004, Accuracy: 0.8272\n",
      "Epoch [19/100], Loss: 0.6886, Accuracy: 0.8423\n",
      "Epoch [20/100], Loss: 0.5905, Accuracy: 0.8660\n",
      "Epoch [21/100], Loss: 0.5075, Accuracy: 0.8796\n",
      "Epoch [22/100], Loss: 0.4440, Accuracy: 0.8967\n",
      "Epoch [23/100], Loss: 0.3883, Accuracy: 0.9133\n",
      "Epoch [24/100], Loss: 0.3390, Accuracy: 0.9257\n",
      "Epoch [25/100], Loss: 0.3051, Accuracy: 0.9323\n",
      "Epoch [26/100], Loss: 0.2787, Accuracy: 0.9375\n",
      "Epoch [27/100], Loss: 0.2501, Accuracy: 0.9443\n",
      "Epoch [28/100], Loss: 0.2316, Accuracy: 0.9467\n",
      "Epoch [29/100], Loss: 0.2103, Accuracy: 0.9528\n",
      "Epoch [30/100], Loss: 0.2058, Accuracy: 0.9510\n",
      "Epoch [31/100], Loss: 0.1911, Accuracy: 0.9558\n",
      "Epoch [32/100], Loss: 0.1838, Accuracy: 0.9577\n",
      "Epoch [33/100], Loss: 0.1672, Accuracy: 0.9606\n",
      "Epoch [34/100], Loss: 0.1650, Accuracy: 0.9609\n",
      "Epoch [35/100], Loss: 0.1548, Accuracy: 0.9629\n",
      "Epoch [36/100], Loss: 0.1538, Accuracy: 0.9619\n",
      "Epoch [37/100], Loss: 0.1447, Accuracy: 0.9650\n",
      "Epoch [38/100], Loss: 0.1385, Accuracy: 0.9665\n",
      "Epoch [39/100], Loss: 0.1390, Accuracy: 0.9667\n",
      "Epoch [40/100], Loss: 0.1393, Accuracy: 0.9661\n",
      "Epoch [41/100], Loss: 0.1354, Accuracy: 0.9665\n",
      "Epoch [42/100], Loss: 0.1284, Accuracy: 0.9689\n",
      "Epoch [43/100], Loss: 0.1346, Accuracy: 0.9671\n",
      "Epoch [44/100], Loss: 0.1205, Accuracy: 0.9709\n",
      "Epoch [45/100], Loss: 0.1230, Accuracy: 0.9694\n",
      "Epoch [46/100], Loss: 0.1185, Accuracy: 0.9708\n",
      "Epoch [47/100], Loss: 0.1189, Accuracy: 0.9714\n",
      "Epoch [48/100], Loss: 0.1184, Accuracy: 0.9714\n",
      "Epoch [49/100], Loss: 0.1146, Accuracy: 0.9720\n",
      "Epoch [50/100], Loss: 0.1118, Accuracy: 0.9730\n",
      "Epoch [51/100], Loss: 0.1099, Accuracy: 0.9727\n",
      "Epoch [52/100], Loss: 0.1080, Accuracy: 0.9736\n",
      "Epoch [53/100], Loss: 0.1078, Accuracy: 0.9734\n",
      "Epoch [54/100], Loss: 0.1073, Accuracy: 0.9734\n",
      "Epoch [55/100], Loss: 0.1040, Accuracy: 0.9740\n",
      "Epoch [56/100], Loss: 0.1034, Accuracy: 0.9740\n",
      "Epoch [57/100], Loss: 0.1034, Accuracy: 0.9738\n",
      "Epoch [58/100], Loss: 0.1037, Accuracy: 0.9742\n",
      "Epoch [59/100], Loss: 0.0993, Accuracy: 0.9753\n",
      "Epoch [60/100], Loss: 0.1011, Accuracy: 0.9750\n",
      "Epoch [61/100], Loss: 0.0978, Accuracy: 0.9756\n",
      "Epoch [62/100], Loss: 0.0999, Accuracy: 0.9747\n",
      "Epoch [63/100], Loss: 0.0989, Accuracy: 0.9751\n",
      "Epoch [64/100], Loss: 0.0981, Accuracy: 0.9752\n",
      "Epoch [65/100], Loss: 0.0932, Accuracy: 0.9768\n",
      "Epoch [66/100], Loss: 0.0949, Accuracy: 0.9761\n",
      "Epoch [67/100], Loss: 0.0920, Accuracy: 0.9770\n",
      "Epoch [68/100], Loss: 0.0938, Accuracy: 0.9763\n",
      "Epoch [69/100], Loss: 0.0902, Accuracy: 0.9770\n",
      "Epoch [70/100], Loss: 0.0888, Accuracy: 0.9773\n",
      "Epoch [71/100], Loss: 0.0893, Accuracy: 0.9773\n",
      "Epoch [72/100], Loss: 0.0873, Accuracy: 0.9776\n",
      "Epoch [73/100], Loss: 0.0884, Accuracy: 0.9775\n",
      "Epoch [74/100], Loss: 0.0884, Accuracy: 0.9777\n",
      "Epoch [75/100], Loss: 0.0892, Accuracy: 0.9771\n",
      "Epoch [76/100], Loss: 0.0864, Accuracy: 0.9782\n",
      "Epoch [77/100], Loss: 0.0869, Accuracy: 0.9780\n",
      "Epoch [78/100], Loss: 0.0872, Accuracy: 0.9778\n",
      "Epoch [79/100], Loss: 0.0867, Accuracy: 0.9780\n",
      "Epoch [80/100], Loss: 0.0846, Accuracy: 0.9777\n",
      "Epoch [81/100], Loss: 0.0851, Accuracy: 0.9781\n",
      "Epoch [82/100], Loss: 0.0833, Accuracy: 0.9788\n",
      "Epoch [83/100], Loss: 0.0817, Accuracy: 0.9791\n",
      "Epoch [84/100], Loss: 0.0805, Accuracy: 0.9790\n",
      "Epoch [85/100], Loss: 0.0815, Accuracy: 0.9788\n",
      "Epoch [86/100], Loss: 0.0821, Accuracy: 0.9791\n",
      "Epoch [87/100], Loss: 0.0809, Accuracy: 0.9795\n",
      "Epoch [88/100], Loss: 0.0813, Accuracy: 0.9796\n",
      "Epoch [89/100], Loss: 0.0807, Accuracy: 0.9792\n",
      "Epoch [90/100], Loss: 0.0815, Accuracy: 0.9793\n",
      "Epoch [91/100], Loss: 0.0809, Accuracy: 0.9795\n",
      "Epoch [92/100], Loss: 0.0783, Accuracy: 0.9799\n",
      "Epoch [93/100], Loss: 0.0800, Accuracy: 0.9793\n",
      "Epoch [94/100], Loss: 0.0797, Accuracy: 0.9796\n",
      "Epoch [95/100], Loss: 0.0789, Accuracy: 0.9799\n",
      "Epoch [96/100], Loss: 0.0798, Accuracy: 0.9795\n",
      "Epoch [97/100], Loss: 0.0791, Accuracy: 0.9794\n",
      "Epoch [98/100], Loss: 0.0778, Accuracy: 0.9800\n",
      "Epoch [99/100], Loss: 0.0791, Accuracy: 0.9794\n",
      "Epoch [100/100], Loss: 0.0800, Accuracy: 0.9794\n",
      "Wall time: 47min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 손실 함수 및 최적화기 설정\n",
    "criterion = nn.CrossEntropyLoss().to(device)  # 다중 클래스 분류용 크로스 엔트로피 손실\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "# 모델 학습\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 순전파\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # 손실 계산\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # 역전파\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # 예측\n",
    "        _, predicted = torch.max(outputs, 1)  # 가장 높은 값의 인덱스를 예측\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4858b3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'lstm_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c8c3514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 3.6403\n",
      "Test Accuracy: 69.97%\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가\n",
    "model = LSTMModel(input_size, hidden_size, num_classes).to(device)\n",
    "model.load_state_dict(torch.load('lstm_model.pth'))\n",
    "model.eval()  # 평가 모드 전환\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        # 예측 결과 계산\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "avg_loss = test_loss / total\n",
    "accuracy = correct / total\n",
    "\n",
    "print(\"Test Loss: {:.4f}\".format(avg_loss))\n",
    "print(\"Test Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066f1ba7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
