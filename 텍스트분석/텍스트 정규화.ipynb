{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd7788b1",
   "metadata": {},
   "source": [
    "# 텍스트 정규화\n",
    "\n",
    "텍스트를 피처로 만들기 위해서는 가공하는 전처리 작업을 해주어야한다. \n",
    "\n",
    "클렌징 : 텍스트 분석에서 방해가되는 불필요한 문자,기호 등을 사전에 제거 \n",
    "\n",
    "토큰화 : 문장을 분리하는 문장 토큰화와 단어를 분리하는 단어 토큰화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70606719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lmj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 문장 토큰화 : 마침표, 개행문자(\\n) 등 문장의 마지막을 뜻하는 기호에 따라 분리하는것이 일반적\n",
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "               You can see it out your window or on your television. \\\n",
    "               You feel it when you go to work, or go to church or pay your taxes.'\n",
    "sentences = sent_tokenize(text = text_sample)\n",
    "print(type(sentences),len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f57f405",
   "metadata": {},
   "source": [
    "sent_toknize 는 각각의 문장으로 구성된 list객체이다. 반환된 list객체가 3개의 문장으로 된 문자열을 가지는것을 알수있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "139ec935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 15\n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "# 단어 토큰화 : 공백, 콤마, 마침표, 개행문자 등으로 단어를 분리하지만 다양한 방식으로도 가능\n",
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = \"The Matrix is everywhere its all around us, here even in this room.\"\n",
    "words = word_tokenize(sentence)\n",
    "print(type(words), len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e9dbb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "# 문장 토큰화와 단어 토큰화를 조합\n",
    "def tokenize_text(text):\n",
    "    # 문장별로 분리 토큰\n",
    "    sentences = sent_tokenize(text)\n",
    "    # 분리된 문장별 단어 토큰화\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "#여러 문장들에 대해 문장별 단어 토큰화 수행. \n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(word_tokens),len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2676abd9",
   "metadata": {},
   "source": [
    "3개로 토큰화된 문장이 다시 각각 단어로 토큰화되었다.\n",
    "\n",
    "하지만 이처럼 문장을 단어별로 하나씩 토큰화 할 경우 문맥적 의미가 무시된다.\n",
    "\n",
    "이를 보완하기 위해 n-gram 이라는 것을 도입했는데, 이는 연속된 n개의 단어를 하나의 토큰화 단위로 분리해내는 것 이다.\n",
    "\n",
    "ex) 2-gram : \"Agent Smith knock the door\" -> (Agent Smith), (Smith knock), (knock the), (the door)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dcd650",
   "metadata": {},
   "source": [
    "# 스톱 워드 제거\n",
    "\n",
    "스톱 워드는 분석에 큰 의미가 없는 단어들 지칭한다. (is, the, a, will 등)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1202c972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stop words 갯수: 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lmj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# nltk의 스톱워드 목록\n",
    "nltk.download('stopwords')\n",
    "print('영어 stop words 갯수:',len(nltk.corpus.stopwords.words('english')))\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7687fea3",
   "metadata": {},
   "source": [
    "총 179개의 영어 stopwords가 있고 그중 20개만 살펴보았다.\n",
    "\n",
    "이를 이용하여 stopwords를 제거하고 의미있는 단어만 추출해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7abbb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "# 위 예제의 3개의 문장별로 얻은 word_tokens list 에 대해 stop word 제거 Loop\n",
    "for sentence in word_tokens:\n",
    "    filtered_words=[]\n",
    "    # 개별 문장별로 tokenize된 sentence list에 대해 stop word 제거 Loop\n",
    "    for word in sentence:\n",
    "        #소문자로 모두 변환합니다. \n",
    "        word = word.lower()\n",
    "        # tokenize 된 개별 word가 stop words 들의 단어에 포함되지 않으면 word_tokens에 추가\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)\n",
    "    \n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc549f5",
   "metadata": {},
   "source": [
    "# Stemming과 Lemmatization\n",
    "\n",
    "이는 문법적 또는 의미적으로 변화하는 단어의 원형을 찾는 것\n",
    "\n",
    "Lemmatization이 좀 더 정교하지만 시간이 오래 걸림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d38e03df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n",
      "fant fanciest\n"
     ]
    }
   ],
   "source": [
    "# stemming 기능을 이용하여 단어의 원형을 찾자\n",
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'),stemmer.stem('amuses'),stemmer.stem('amused'))\n",
    "print(stemmer.stem('happier'),stemmer.stem('happiest'))\n",
    "print(stemmer.stem('fancier'),stemmer.stem('fanciest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8072bbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lmj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happy\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "# lemmatization 기능을 이용하여 단어의 원형을 찾자\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#lemmatization는 정확한 원형을 추출하기 위해 단어의 품사도 입력해주어야 함\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('amusing','v'),lemma.lemmatize('amuses','v'),lemma.lemmatize('amused','v'))\n",
    "print(lemma.lemmatize('happier','a'),lemma.lemmatize('happiest','a'))\n",
    "print(lemma.lemmatize('fancier','a'),lemma.lemmatize('fanciest','a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fde2560",
   "metadata": {},
   "source": [
    "lemmetization을 이용한 원형 추출이 더 정확한것을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a7cd32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
